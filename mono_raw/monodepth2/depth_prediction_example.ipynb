{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example prediction on a single test image\n",
    "\n",
    "This notebook gives example code to make a single disparity prediction for one test image.\n",
    "\n",
    "The file `test_simple.py` shows a more complete version of this code, which additionally:\n",
    "- Can run on GPU or CPU (this notebook only runs on CPU)\n",
    "- Can predict for a whole folder of images, not just a single image\n",
    "- Saves predictions to `.npy` files and disparity images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'monodepth2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/hamza/workspace/Depth/23.mono/monodepth2/depth_prediction_example.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hamza/workspace/Depth/23.mono/monodepth2/depth_prediction_example.ipynb#ch0000001?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamza/workspace/Depth/23.mono/monodepth2/depth_prediction_example.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamza/workspace/Depth/23.mono/monodepth2/depth_prediction_example.ipynb#ch0000001?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnetworks\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamza/workspace/Depth/23.mono/monodepth2/depth_prediction_example.ipynb#ch0000001?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m download_model_if_doesnt_exist\n",
      "File \u001b[0;32m~/workspace/Depth/23.mono/monodepth2/networks/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresnet_encoder\u001b[39;00m \u001b[39mimport\u001b[39;00m ResnetEncoder\n\u001b[0;32m----> <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdepth_decoder\u001b[39;00m \u001b[39mimport\u001b[39;00m DepthDecoder\n\u001b[1;32m      <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpose_decoder\u001b[39;00m \u001b[39mimport\u001b[39;00m PoseDecoder\n\u001b[1;32m      <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/__init__.py?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpose_cnn\u001b[39;00m \u001b[39mimport\u001b[39;00m PoseCNN\n",
      "File \u001b[0;32m~/workspace/Depth/23.mono/monodepth2/networks/depth_decoder.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/depth_decoder.py?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/depth_decoder.py?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderedDict\n\u001b[0;32m---> <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/depth_decoder.py?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmonodepth2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnetworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/depth_decoder.py?line=16'>17</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDepthDecoder\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     <a href='file:///Users/hamza/workspace/Depth/23.mono/monodepth2/networks/depth_decoder.py?line=17'>18</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, num_ch_enc, scales\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m), num_output_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, use_skips\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'monodepth2'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import networks\n",
    "from utils import download_model_if_doesnt_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up network and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mono_640x192\"\n",
    "\n",
    "download_model_if_doesnt_exist(model_name)\n",
    "encoder_path = os.path.join(\"models\", model_name, \"encoder.pth\")\n",
    "depth_decoder_path = os.path.join(\"models\", model_name, \"depth.pth\")\n",
    "\n",
    "# LOADING PRETRAINED MODEL\n",
    "encoder = networks.ResnetEncoder(18, False)\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "loaded_dict = torch.load(depth_decoder_path, map_location='cpu')\n",
    "depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "encoder.eval()\n",
    "depth_decoder.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the test image and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"assets/test_image.jpg\"\n",
    "\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "\n",
    "feed_height = loaded_dict_enc['height']\n",
    "feed_width = loaded_dict_enc['width']\n",
    "input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = encoder(input_image_pytorch)\n",
    "    outputs = depth_decoder(features)\n",
    "\n",
    "disp = outputs[(\"disp\", 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_resized = torch.nn.functional.interpolate(disp,\n",
    "    (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# Saving colormapped depth image\n",
    "disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input\", fontsize=22)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "plt.title(\"Disparity prediction\", fontsize=22)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36e103213bfac07b4a1b3d76344c58d1f91de3835d0e5595567e30d0e3abab09"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
